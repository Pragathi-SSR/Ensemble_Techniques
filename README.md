# Ensemble_Techniques
The objective of this ensemble learning project is to improve the predictive performance of machine learning models by combining multiple base models to create a more robust and accurate final model. By leveraging techniques such as bagging, boosting the project aims to reduce overfitting, enhance generalization across various datasets.

This ensemble learning project focuses on harnessing the power of multiple machine learning models to achieve superior predictive accuracy. By implementing and comparing various ensemble methods like bagging, boosting the project seeks to minimize the limitations of individual models, such as overfitting and variance. Through the aggregation of diverse models, the project aims to create a more reliable and generalizable predictive system, ultimately improving the performance and robustness of the final model across different datasets and problem domains.


## Features
- Treating missing values and unexpected values with a suitable approach.
- Understanding the significant insights in the data by exploratory data analysis.
- Applying ensemble methods such as bagging, boosting, and stacking to combine base models, enhancing predictive performance and model robustness.
- Comparing accuracies and other scores to get the best model for deployment.

## Dataset

- Telecom Churn Prediction Dataset.



https://www.kaggle.com/datasets/blastchar/telco-customer-churn



## Tools and Techniques

- Python
- Numpy
- Pandas
- Matplotlib
- Seaborn
- KNN, SVM
- Bagging, boosting
- RandomForest, DecisionTree, GradientBoosting, AdaBoost, SVM.
- Bootstrap Aggregation.
## Analysis


### Telecom customer churn prediction


                                                                       ![image](https://github.com/user-attachments/assets/1f7ffd54-1d8d-4bcb-aadd-82a4a1d32710)




- Models with hyper parameter tuning using GridSerachCV  are giving good Preciison, Recall, Accuracy and F1_score.
- Random Forest Classifier is giving the best scores according to the classification problem after tuning.

- Best Model scores are:

- **Accuracy:** 0.75

- **Precision:** 0.52 

- **Recall:** 0.63

- **F1-Score:** 0.57

- **AUC_roc_Score:** 0.71






